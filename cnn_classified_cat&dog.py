# -*- coding: utf-8 -*-
"""CNN_Classified_Cat&Dog

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16iFYrzpYp5Ty6X0QTeb1zFMB8mjP1uEf
"""

# _*_ coding: utf-8 _*_
#!/usr/bin/env python

"""
@Project Name  ColabProject
@File Name:    CNN_Classified_Cat&Dog
@Software:     Google Colab
@Time:         12/May/2022
@Author:       zaicy
@contact:      zaicyxu@gmail.com
@version:      1.0
@Description:  Simple CNN model to classified cat and dog.
"""

import numpy as np
import os
import cv2
import tensorflow as tf


os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # 0 meas the first GPU
tf.config.set_soft_device_placement = False
tf.config.experimental.set_memory_growth = True
gpus = tf.config.experimental.list_physical_devices('GPU') # Get all GPU
print("gpus:", gpus)

if gpus:
    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])
    #logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    #print(len(gpus), len(logical_gpus), 'Logical gpus')


# Random
np.random.seed(10)

train_path = r'train'

# Getting labeled dictionary
label_dict = {i: j for i, j in enumerate(os.listdir(train_path))}

# Setting train set and test set.
train_imgs = []
train_labels = []
test_imgs = []
test_labels = []

train_normalize_imgs = []
test_normalize_imgs = []

train_num = 10000       # use the train images's mun
train_proportion = 0.9  # classify train and test data set
# Getting train data and label
for label, path in enumerate(os.listdir(train_path)):
    images = os.listdir(os.path.join(train_path, path))
    images = images[:train_num]
    for index, img in enumerate(images):
        # Get images' path
        img = cv2.imread(os.path.join(train_path, path, img))
        img = cv2.resize(img, (100, 100), interpolation=cv2.INTER_AREA)
        # Judging train data set and divide the set into train and test.
        if index < train_num * train_proportion:
            train_labels.append(label)
            train_imgs.append(img)
            train_normalize_imgs.append(img.astype('float64'))
        else:
            test_labels.append(label)
            test_imgs.append(img)
            test_normalize_imgs.append(img.astype('float64'))


# Convert data format.
train_imgs = np.array(train_imgs)
train_labels = np.array(train_labels)
test_imgs = np.array(test_imgs)
test_labels = np.array(test_labels)

train_normalize_imgs = np.array(train_normalize_imgs)
test_normalize_imgs = np.array(test_normalize_imgs)

# Output shape
print("train data:", 'images:', train_imgs.shape, " labels:", train_labels.shape)
print("test  data:", 'images:', test_imgs.shape, " labels:", test_labels.shape)

# Turn into one-hot.
from keras.utils import np_utils

train_labels_OneHot = np_utils.to_categorical(train_labels)
test_labels_OneHot = np_utils.to_categorical(test_labels)


# Estublish CNN nework work model.
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense

model = Sequential()

# Convoluntion 1. & Pooling 1.
model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(100, 100, 3),
                 activation='relu', padding='same'))

model.add(Conv2D(filters=32, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Convoluntion 2. & Pooling 2.
model.add(Conv2D(filters=64, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(Conv2D(filters=64, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Convoluntion 3. & Pooling 3.

model.add(Conv2D(filters=128, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(Conv2D(filters=128, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Convoluntion 4. & Pooling 4.
model.add(Conv2D(filters=256, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(Conv2D(filters=256, kernel_size=(3, 3),
                 activation='relu', padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2)))

# Build neural network(flat layer, hidden layer, output layer)
model.add(Flatten())
model.add(Dropout(0.15))
model.add(Dense(1000, activation='relu'))
model.add(Dropout(0.15))
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.15))
model.add(Dense(2, activation='softmax'))

#Output model list.
print(model.summary())

# Training model.
epochs = 30  # epochs
model.compile(loss='categorical_crossentropy', optimizer='adam',
              metrics=['accuracy'])

train_history = model.fit(train_normalize_imgs, train_labels_OneHot,
                          validation_split=0.0005,shuffle=True,
                          epochs=epochs, batch_size=256)

# Evaluate the model accuracy.
import matplotlib.pyplot as plt


def show_train_history(train_history, train_acc, test_acc):
    plt.plot(train_history.history[train_acc])
    plt.plot(train_history.history[test_acc])
    plt.title('Train History')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['train', 'test'], loc='upper left')
    plt.show()


show_train_history(train_history, 'accuracy', 'val_accuracy')
show_train_history(train_history, 'loss', 'val_loss')

scores = model.evaluate(test_normalize_imgs,
                        test_labels_OneHot, verbose=0)


print("scores:", scores)

# Prediction.
prediction = model.predict_classes(test_normalize_imgs)
from collections import Counter
result = Counter(prediction[1000:])
print('cat 识别数量：' , result)
result = Counter(prediction[1000:])
print('dog 识别数量：' , result)

"""
def plot_images_labels_prediction(images, labels, prediction,
                                  idx, num=10):
    fig = plt.gcf()
    fig.set_size_inches(12, 14)
    if num > 25: num = 25
    for i in range(0, num):
        ax = plt.subplot(5, 5, 1 + i)
        ax.imshow(images[idx], cmap='binary')

        title = str(i) + ',' + label_dict[labels[i]]
        if len(prediction) > 0:
            title += '=>' + label_dict[prediction[i]]
        ax.set_title(title, fontsize=10)
        ax.set_xticks([]);
        ax.set_yticks([])
        idx += 1
    plt.show()


plot_images_labels_prediction(test_normalize_imgs, test_labels,
                              prediction, 0, 10)
"""
# Predicted probability
Predicted_Probability = model.predict(test_normalize_imgs)
print("Predicted_Probability:", Predicted_Probability)


def show_Predicted_Probability(X_img, Predicted_Probability, i):
    plt.figure(figsize=(2, 2))
    plt.imshow(np.reshape(test_imgs[i], (100, 100, 3)))
    plt.show()
    for j in range(2):
        print(label_dict[j] + ' Probability:%1.9f' % (Predicted_Probability[i][j]))


show_Predicted_Probability(test_imgs, Predicted_Probability, 0)



# Load images
img_names = ['test_cat.jpg', 'test_dog.jpg']

imgs = []
for img_name in img_names:
    img = cv2.imread(img_name)
    img = cv2.resize(img, (100, 100), interpolation=cv2.INTER_AREA)
    img = img.astype('float32')
    imgs.append(img)

imgs = np.array(imgs) / 255

predictions = model.predict_classes(imgs)
print(predictions)


# Save model and json files.
open('Keras_Cifar_CNN_Conv3_architecture.json', 'w').write(model.to_json())
model.save('CNN.h5')
model.save_weights('Keras_Cifar_CNN_Conv3_weights.h5', overwrite=True)